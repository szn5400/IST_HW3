{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IST_HW3_Final_v1_fmnist.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMe97Ons5wqH1x84kMh6NDV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/szn5400/IST_HW3/blob/main/IST_HW3_Final_v1_fmnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P3O8RS04ZtLm"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = x_train[50000:60000]\n",
        "y_val = y_train[50000:60000]\n",
        "x_train = x_train[0:50000]\n",
        "y_train = y_train[0:50000]"
      ],
      "metadata": {
        "id": "wEYgkH72aDkg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Define the One-hot Encoder\n",
        "ohe = preprocessing.OneHotEncoder()\n",
        "\n",
        "# Load MNIST data\n",
        "#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape data\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "y_val = y_val.reshape(-1, 1)\n",
        "y_test = y_test.reshape(-1, 1)\n",
        "\n",
        "# Fit and transform training data\n",
        "ohe.fit(y_train)\n",
        "transformed_train = ohe.transform(y_train).toarray()\n",
        "\n",
        "# Fit and transform training data\n",
        "ohe.fit(y_val)\n",
        "transformed_val = ohe.transform(y_val).toarray()\n",
        "\n",
        "# Fit and transform testing data\n",
        "ohe.fit(y_test)\n",
        "transformed_test = ohe.transform(y_test).toarray()"
      ],
      "metadata": {
        "id": "LZk2S_77aRJQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('X_train: ' + str(x_train.shape))\n",
        "print('Y_train: ' + str(transformed_train.shape))\n",
        "print('X_val: ' + str(x_val.shape))\n",
        "print('Y_val: ' + str(transformed_val.shape))\n",
        "print('X_test:  '  + str(x_test.shape))\n",
        "print('Y_test:  '  + str(transformed_test.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdO83eKGaTy-",
        "outputId": "1870e949-8a68-48ec-e151-5d184d1ad6c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: (50000, 28, 28)\n",
            "Y_train: (50000, 10)\n",
            "X_val: (10000, 28, 28)\n",
            "Y_val: (10000, 10)\n",
            "X_test:  (10000, 28, 28)\n",
            "Y_test:  (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X_mod = x_train.reshape(50000, 784)\n",
        "train_y_mod = transformed_train\n",
        "val_X_mod = x_val.reshape(10000, 784)\n",
        "val_y_mod = transformed_val\n",
        "test_X_mod = x_test.reshape(10000, 784)\n",
        "test_y_mod = transformed_test"
      ],
      "metadata": {
        "id": "IaHf1co4aVhq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size_input = 784\n",
        "size_hidden1 = 512\n",
        "size_hidden2 = 256\n",
        "size_output = 10\n",
        "number_of_train_examples = 50000\n",
        "number_of_val_examples = 10000\n",
        "number_of_test_examples = 10000"
      ],
      "metadata": {
        "id": "q6x4SlJMaXse"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "X_train = tf.keras.utils.normalize(train_X_mod, axis=1)\n",
        "y_train = train_y_mod\n",
        "X_val = tf.keras.utils.normalize(val_X_mod, axis=1)\n",
        "y_val = val_y_mod\n",
        "X_test = test_X_mod\n",
        "y_test = test_y_mod "
      ],
      "metadata": {
        "id": "hwXy_IxoaZTk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "np.random.seed(43)\n",
        "tf.random.set_seed(43)\n",
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(128)\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(4)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)"
      ],
      "metadata": {
        "id": "UWruYXyFaa5z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden1, size_hidden2, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of hidden layer 1\n",
        "    size_hidden2: int, size of hodden layer 2\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "    # Initialize weights between input layer and hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    self.t=0\n",
        "    self.mt=[tf.zeros_like(self.W1,dtype=tf.float32), tf.zeros_like(self.W2,dtype=tf.float32),tf.zeros_like(self.W3,dtype=tf.float32),tf.zeros_like(self.b1,dtype=tf.float32),tf.zeros_like(self.b2,dtype=tf.float32),tf.zeros_like(self.b3,dtype=tf.float32)]\n",
        "    self.vt=[tf.zeros_like(self.W1,dtype=tf.float32), tf.zeros_like(self.W2,dtype=tf.float32),tf.zeros_like(self.W3,dtype=tf.float32),tf.zeros_like(self.b1,dtype=tf.float32),tf.zeros_like(self.b2,dtype=tf.float32),tf.zeros_like(self.b3,dtype=tf.float32)]\n",
        "    self.ut=[tf.zeros_like(self.W1,dtype=tf.float32), tf.zeros_like(self.W2,dtype=tf.float32),tf.zeros_like(self.W3,dtype=tf.float32),tf.zeros_like(self.b1,dtype=tf.float32),tf.zeros_like(self.b2,dtype=tf.float32),tf.zeros_like(self.b3,dtype=tf.float32)]\n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    #return tf.losses.mean_squared_error(y_true_tf, y_pred_tf)\n",
        "    #print(y_true_tf)\n",
        "    #print(y_pred_tf)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "    loss_val = cce(y_true_tf, y_pred_tf)\n",
        "    regularizer = tf.nn.l2_loss(self.W1)+tf.nn.l2_loss(self.W2)\n",
        "    loss_val = tf.reduce_mean(loss_val + 0.01 * regularizer)\n",
        "    \n",
        "    #print(loss_val)\n",
        "    return loss_val\n",
        "\n",
        "  def accuracy(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    #y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(tf.reshape(y_pred, (-1, self.size_output)), dtype=tf.float32)\n",
        "    print(y_true_tf)\n",
        "    print(y_pred_tf)\n",
        "    ## CALCULATING COST AND ACCURACY\n",
        "    #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_preds, labels=y))\n",
        "    #optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost)\n",
        "    correct_pred = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
        "    #print(correct_pred)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
        "    #print(ac)\n",
        "    return accuracy\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    #y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(tf.reshape(y_pred, (-1, self.size_output)), dtype=tf.float32)    \n",
        "    correct_pred = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
        "    #print(correct_pred)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
        "    #print(accuracy)\n",
        "    return accuracy\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    '''\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "    '''\n",
        "    self.t=self.t+1\n",
        "    alpha = 1e-3\n",
        "    beta_1=0.9\n",
        "    beta_2=0.999\n",
        "    beta_3=0.999987\n",
        "    eps=1e-8\n",
        "    eps2 = 1e-1 \n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    #grads = grads+eps2\n",
        "    #print(grads)\n",
        "    #print(len(grads[0]))\n",
        "    #print(len(grads[1]))\n",
        "    #print(len(self.m[0]))\n",
        "    #print(len(self.m[1]))\n",
        "\n",
        "    self.mt=[m_val*beta_1+(1-beta_1)*g for m_val, g in zip(self.mt, grads) ]\n",
        "    self.vt=[v_val*beta_3+(1-beta_3)*(g**3) for v_val, g in zip(self.vt, grads) ]\n",
        "    self.ut=[u_val*beta_2+(1-beta_2)*(g**2) for u_val, g in zip(self.ut, grads) ]\n",
        "    '''\n",
        "    print('MT')\n",
        "    print(self.mt)\n",
        "    print('VT')\n",
        "    print(self.vt)\n",
        "    print('UT')\n",
        "    print(self.ut)\n",
        "    '''\n",
        "\n",
        "    mt_hat=[m_val/(1-(beta_1**self.t)) for m_val in self.mt]\n",
        "    vt_hat=[v_val/(1-(beta_3**self.t)) for v_val in self.vt]\n",
        "    ut_hat=[u_val/(1-(beta_2**self.t)) for u_val in self.ut]\n",
        "    \n",
        "    \n",
        "    dws_new = [(alpha * m_val /(np.sqrt(np.abs(v_val))+(np.power(np.abs(u_val),0.3333)*eps)+eps2)) for m_val, v_val, u_val in zip(mt_hat,vt_hat,ut_hat)]\n",
        "    #dws_new = [(alpha * m_val /(eps2)) for m_val, v_val, u_val in zip(mt_hat,vt_hat,ut_hat)]\n",
        "    #print('dws_new')\n",
        "    #print(dws_new)\n",
        "    Wt = [wt - d for wt,d in zip(self.variables, dws_new)]\n",
        "    #print(Wt)\n",
        "    W1 = self.variables[0]\n",
        "    W1.assign(Wt[0])\n",
        "    W2 = self.variables[1]\n",
        "    W2.assign(Wt[1])\n",
        "    W3 = self.variables[2]\n",
        "    W3.assign(Wt[2])\n",
        "    b1 = self.variables[3]\n",
        "    b1.assign(Wt[3])\n",
        "    b2 = self.variables[4]\n",
        "    b2.assign(Wt[4])\n",
        "    b3 = self.variables[5]\n",
        "    b3.assign(Wt[5])       \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    hhat1_1 = tf.nn.dropout(hhat1, 0.25)\n",
        "\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1_1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    hhat2_1 = tf.nn.dropout(hhat2, 0.25)\n",
        "    \n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2_1, self.W3) + self.b3\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    output = tf.nn.softmax(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "7NyBF0Ryacyp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 40"
      ],
      "metadata": {
        "id": "j46eJdBNajmj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_output, device='cpu')\n",
        "\n",
        "# Array to store accuracy and loss\n",
        "loss_with_epoch = []\n",
        "acc_with_epoch = []\n",
        "loss_with_epoch_val = []\n",
        "acc_with_epoch_val = []\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  ac = 0\n",
        "  count = 0\n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(1234)).batch(20)\n",
        "  val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).shuffle(25, seed=epoch*(1234)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_cpu.forward(inputs)\n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "    ac = ac+mlp_on_cpu.accuracy(preds, outputs)\n",
        "    #ac = mlp_on_cpu.accuracy(preds, outputs)\n",
        "    count += 1\n",
        "  print('Number of Epoch = {} - Average celoss:= {}- Acc:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0], ac/count))\n",
        "  loss_with_epoch.append(np.sum(loss_total) / X_train.shape[0])\n",
        "  acc_with_epoch.append(ac/count)\n",
        "  ac = 0\n",
        "  count = 0\n",
        "  for inputs, outputs in val_ds:\n",
        "    preds = mlp_on_cpu.forward(inputs)\n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "    ac = ac+mlp_on_cpu.accuracy(preds, outputs)\n",
        "    #ac = mlp_on_cpu.accuracy(preds, outputs)\n",
        "    count += 1\n",
        "  #print('Number of Epoch = {} - Average celoss:= {}- Acc:= {} '.format(epoch + 1, np.sum(loss_total) / X_val.shape[0], ac/count))\n",
        "  loss_with_epoch_val.append(np.sum(loss_total) / X_train.shape[0])\n",
        "  acc_with_epoch_val.append(ac/count)\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQhvGrnualgS",
        "outputId": "3dca1c86-70c4-497f-a937-8bf59485b585"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Average celoss:= 1.11823171875- Acc:= 0.5690006017684937 \n",
            "Number of Epoch = 2 - Average celoss:= 0.6244725- Acc:= 0.718879759311676 \n",
            "Number of Epoch = 3 - Average celoss:= 0.363000859375- Acc:= 0.7475004196166992 \n",
            "Number of Epoch = 4 - Average celoss:= 0.219649453125- Acc:= 0.7621800899505615 \n",
            "Number of Epoch = 5 - Average celoss:= 0.1408896875- Acc:= 0.7713409066200256 \n",
            "Number of Epoch = 6 - Average celoss:= 0.09734919921875- Acc:= 0.7761195302009583 \n",
            "Number of Epoch = 7 - Average celoss:= 0.0731150146484375- Acc:= 0.7815207839012146 \n",
            "Number of Epoch = 8 - Average celoss:= 0.05961400390625- Acc:= 0.7863208055496216 \n",
            "Number of Epoch = 9 - Average celoss:= 0.0519948046875- Acc:= 0.7898606061935425 \n",
            "Number of Epoch = 10 - Average celoss:= 0.0474983544921875- Acc:= 0.7895606160163879 \n",
            "Number of Epoch = 11 - Average celoss:= 0.04482083984375- Acc:= 0.7918201684951782 \n",
            "Number of Epoch = 12 - Average celoss:= 0.043196826171875- Acc:= 0.7943209409713745 \n",
            "Number of Epoch = 13 - Average celoss:= 0.0420795458984375- Acc:= 0.7964999675750732 \n",
            "Number of Epoch = 14 - Average celoss:= 0.041352265625- Acc:= 0.7980802059173584 \n",
            "Number of Epoch = 15 - Average celoss:= 0.040808408203125- Acc:= 0.799281120300293 \n",
            "Number of Epoch = 16 - Average celoss:= 0.040271162109375- Acc:= 0.8015004992485046 \n",
            "Number of Epoch = 17 - Average celoss:= 0.03996284912109375- Acc:= 0.8016408681869507 \n",
            "Number of Epoch = 18 - Average celoss:= 0.0396368505859375- Acc:= 0.8027005791664124 \n",
            "Number of Epoch = 19 - Average celoss:= 0.03934514892578125- Acc:= 0.8028203845024109 \n",
            "Number of Epoch = 20 - Average celoss:= 0.0390579150390625- Acc:= 0.8064001202583313 \n",
            "Number of Epoch = 21 - Average celoss:= 0.0387884130859375- Acc:= 0.8082005977630615 \n",
            "Number of Epoch = 22 - Average celoss:= 0.038561005859375- Acc:= 0.8081204295158386 \n",
            "Number of Epoch = 23 - Average celoss:= 0.03845262939453125- Acc:= 0.806781530380249 \n",
            "Number of Epoch = 24 - Average celoss:= 0.038165244140625- Acc:= 0.8111200332641602 \n",
            "Number of Epoch = 25 - Average celoss:= 0.03792848388671875- Acc:= 0.8108798861503601 \n",
            "Number of Epoch = 26 - Average celoss:= 0.037783876953125- Acc:= 0.810640275478363 \n",
            "Number of Epoch = 27 - Average celoss:= 0.03763421630859375- Acc:= 0.8104589581489563 \n",
            "Number of Epoch = 28 - Average celoss:= 0.03741537353515625- Acc:= 0.8103599548339844 \n",
            "Number of Epoch = 29 - Average celoss:= 0.03728278564453125- Acc:= 0.8120015859603882 \n",
            "Number of Epoch = 30 - Average celoss:= 0.0371114013671875- Acc:= 0.814300537109375 \n",
            "Number of Epoch = 31 - Average celoss:= 0.03696371826171875- Acc:= 0.8136802315711975 \n",
            "Number of Epoch = 32 - Average celoss:= 0.036833271484375- Acc:= 0.8137610554695129 \n",
            "Number of Epoch = 33 - Average celoss:= 0.036669189453125- Acc:= 0.8155807256698608 \n",
            "Number of Epoch = 34 - Average celoss:= 0.03661459228515625- Acc:= 0.81572026014328 \n",
            "Number of Epoch = 35 - Average celoss:= 0.03642225341796875- Acc:= 0.8157601356506348 \n",
            "Number of Epoch = 36 - Average celoss:= 0.03628287841796875- Acc:= 0.8168202638626099 \n",
            "Number of Epoch = 37 - Average celoss:= 0.0361623291015625- Acc:= 0.8179005980491638 \n",
            "Number of Epoch = 38 - Average celoss:= 0.036043447265625- Acc:= 0.8186402916908264 \n",
            "Number of Epoch = 39 - Average celoss:= 0.03596052978515625- Acc:= 0.8175402283668518 \n",
            "Number of Epoch = 40 - Average celoss:= 0.0358149365234375- Acc:= 0.8181008100509644 \n",
            "\n",
            "Total time taken (in seconds): 9361.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rUGCM6vwapSO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}