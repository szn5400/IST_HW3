{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08279579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3bc557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Define the One-hot Encoder\n",
    "ohe = preprocessing.OneHotEncoder()\n",
    "\n",
    "# Load MNIST data\n",
    "#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape data\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "# Fit and transform training data\n",
    "ohe.fit(y_train)\n",
    "transformed_train = ohe.transform(y_train).toarray()\n",
    "\n",
    "# Fit and transform testing data\n",
    "ohe.fit(y_test)\n",
    "transformed_test = ohe.transform(y_test).toarray()\n",
    "\n",
    "x_train = x_train[0:50000, :]\n",
    "#y_train = transformed_train[0:50000,:]\n",
    "transformed_train = transformed_train[0:50000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6532c083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (50000, 28, 28)\n",
      "Y_train: (50000, 10)\n",
      "X_test:  (10000, 28, 28)\n",
      "Y_test:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print('X_train: ' + str(x_train.shape))\n",
    "print('Y_train: ' + str(transformed_train.shape))\n",
    "print('X_test:  '  + str(x_test.shape))\n",
    "print('Y_test:  '  + str(transformed_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddc6d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_mod = x_train.reshape(50000, 784)\n",
    "train_y_mod = transformed_train\n",
    "test_X_mod = x_test.reshape(10000, 784)\n",
    "test_y_mod = transformed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ae6571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_input = 784\n",
    "size_hidden1 = 512\n",
    "size_hidden2 = 256\n",
    "size_output = 10\n",
    "number_of_train_examples = 50000\n",
    "number_of_test_examples = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa318938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "X_train = tf.keras.utils.normalize(train_X_mod, axis=1)\n",
    "y_train = train_y_mod\n",
    "X_test = test_X_mod\n",
    "y_test = test_y_mod "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d693b7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "np.random.seed(43)\n",
    "tf.random.set_seed(43)\n",
    "# Split dataset into batches\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(128)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1db7f1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "  def __init__(self, size_input, size_hidden1, size_hidden2, size_output, device=None):\n",
    "    \"\"\"\n",
    "    size_input: int, size of input layer\n",
    "    size_hidden1: int, size of hidden layer 1\n",
    "    size_hidden2: int, size of hodden layer 2\n",
    "    size_output: int, size of output layer\n",
    "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
    "    \"\"\"\n",
    "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_output, self.device =\\\n",
    "    size_input, size_hidden1, size_hidden2, size_output, device\n",
    "    \n",
    "    # Initialize weights between input layer and hidden layer\n",
    "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1))\n",
    "    # Initialize biases for hidden layer\n",
    "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden1]))\n",
    "    # Initialize weights between input layer and hidden layer\n",
    "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
    "    # Initialize biases for hidden layer\n",
    "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden2]))\n",
    "     # Initialize weights between hidden layer and output layer\n",
    "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output],stddev=0.1))\n",
    "    # Initialize biases for output layer\n",
    "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
    "    \n",
    "    # Define variables to be updated during backpropagation\n",
    "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
    "    \n",
    "  def forward(self, X):\n",
    "    \"\"\"\n",
    "    forward pass\n",
    "    X: Tensor, inputs\n",
    "    \"\"\"\n",
    "    if self.device is not None:\n",
    "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
    "        self.y = self.compute_output(X)\n",
    "    else:\n",
    "      self.y = self.compute_output(X)\n",
    "      \n",
    "    return self.y\n",
    "  \n",
    "  def loss(self, y_pred, y_true):\n",
    "    '''\n",
    "    y_pred - Tensor of shape (batch_size, size_output)\n",
    "    y_true - Tensor of shape (batch_size, size_output)\n",
    "    '''\n",
    "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
    "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "    #return tf.losses.mean_squared_error(y_true_tf, y_pred_tf)\n",
    "    #print(y_true_tf)\n",
    "    #print(y_pred_tf)\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "    loss_val = cce(y_true_tf, y_pred_tf)\n",
    "    #regularizer = tf.nn.l2_loss(self.W1)+tf.nn.l2_loss(self.W2)\n",
    "    #loss_val = tf.reduce_mean(loss_val + 0.01 * regularizer)\n",
    "    \n",
    "    #print(loss_val)\n",
    "    return loss_val\n",
    "\n",
    "  def accuracy(self, y_pred, y_true):\n",
    "    '''\n",
    "    y_pred - Tensor of shape (batch_size, size_output)\n",
    "    y_true - Tensor of shape (batch_size, size_output)\n",
    "    '''\n",
    "    '''\n",
    "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
    "    #y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "    y_pred_tf = tf.cast(tf.reshape(y_pred, (-1, self.size_output)), dtype=tf.float32)\n",
    "    print(y_true_tf)\n",
    "    print(y_pred_tf)\n",
    "    ## CALCULATING COST AND ACCURACY\n",
    "    #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_preds, labels=y))\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost)\n",
    "    correct_pred = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
    "    #print(correct_pred)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "    #print(ac)\n",
    "    return accuracy\n",
    "    '''\n",
    "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
    "    #y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "    y_pred_tf = tf.cast(tf.reshape(y_pred, (-1, self.size_output)), dtype=tf.float32)    \n",
    "    correct_pred = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
    "    #print(correct_pred)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "    #print(accuracy)\n",
    "    return accuracy\n",
    "  \n",
    "  def backward(self, X_train, y_train):\n",
    "    \"\"\"\n",
    "    backward pass\n",
    "    \"\"\"\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "    with tf.GradientTape() as tape:\n",
    "      predicted = self.forward(X_train)\n",
    "      current_loss = self.loss(predicted, y_train)\n",
    "    grads = tape.gradient(current_loss, self.variables)\n",
    "    optimizer.apply_gradients(zip(grads, self.variables))\n",
    "        \n",
    "        \n",
    "  def compute_output(self, X):\n",
    "    \"\"\"\n",
    "    Custom method to obtain output tensor during forward pass\n",
    "    \"\"\"\n",
    "    # Cast X to float32\n",
    "    X_tf = tf.cast(X, dtype=tf.float32)\n",
    "    #Remember to normalize your dataset before moving forward\n",
    "    # Compute values in hidden layer1\n",
    "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
    "    hhat1 = tf.nn.relu(what1)\n",
    "    #hhat1_1 = tf.nn.dropout(hhat1, keep_prob)\n",
    "    # Compute values in hidden layer2\n",
    "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
    "    hhat2 = tf.nn.relu(what2)\n",
    "    # Compute output\n",
    "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
    "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
    "    #Second add tf.Softmax(output) and then return this variable\n",
    "    output = tf.nn.softmax(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6eeb2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of epochs\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31b6d00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Epoch = 1 - Average celoss:= 0.1051168359375- Acc:= 0.479139506816864 \n",
      "Number of Epoch = 2 - Average celoss:= 0.075668759765625- Acc:= 0.7422204613685608 \n",
      "Number of Epoch = 3 - Average celoss:= 0.054981630859375- Acc:= 0.7971610426902771 \n",
      "Number of Epoch = 4 - Average celoss:= 0.0419624072265625- Acc:= 0.8236796855926514 \n",
      "Number of Epoch = 5 - Average celoss:= 0.03424661376953125- Acc:= 0.8416399359703064 \n",
      "Number of Epoch = 6 - Average celoss:= 0.02949181396484375- Acc:= 0.8526199460029602 \n",
      "Number of Epoch = 7 - Average celoss:= 0.02637247314453125- Acc:= 0.8610800504684448 \n",
      "Number of Epoch = 8 - Average celoss:= 0.02421305419921875- Acc:= 0.868079423904419 \n",
      "Number of Epoch = 9 - Average celoss:= 0.0226669775390625- Acc:= 0.8734791874885559 \n",
      "Number of Epoch = 10 - Average celoss:= 0.02151366943359375- Acc:= 0.8771976828575134 \n",
      "\n",
      "Total time taken (in seconds): 777.17\n"
     ]
    }
   ],
   "source": [
    "# Initialize model using CPU\n",
    "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_output, device='cpu')\n",
    "\n",
    "# Array to store accuracy and loss\n",
    "loss_with_epoch = []\n",
    "acc_with_epoch = []\n",
    "\n",
    "time_start = time.time()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "  ac = 0\n",
    "  count = 0\n",
    "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
    "  lt = 0\n",
    "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(1234)).batch(20)\n",
    "  for inputs, outputs in train_ds:\n",
    "    preds = mlp_on_cpu.forward(inputs)\n",
    "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
    "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
    "    mlp_on_cpu.backward(inputs, outputs)\n",
    "    ac = ac+mlp_on_cpu.accuracy(preds, outputs)\n",
    "    #ac = mlp_on_cpu.accuracy(preds, outputs)\n",
    "    count += 1\n",
    "  print('Number of Epoch = {} - Average celoss:= {}- Acc:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0], ac/count))\n",
    "  loss_with_epoch.append(np.sum(loss_total) / X_train.shape[0])\n",
    "  acc_with_epoch.append(ac/count)\n",
    "time_taken = time.time() - time_start\n",
    "\n",
    "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
    "#For per epoch_time = Total_Time / Number_of_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bd7424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = [i for i in range(1,21)]\n",
    "line1, = plt.plot(x, acc_with_epoch, label='accr')\n",
    "line2, = plt.plot(x, loss_with_epoch, label='ce loss')\n",
    "#plt.plot(x, acc_with_epoch, label='accr')\n",
    "plt.legend(handles=[line1, line2], loc='best')\n",
    "plt.grid(b=True, color='aqua', alpha=0.6, linestyle='dashdot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9173fbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing that pred output is same as actual output for testing\n",
    "def print_val(x,y):\n",
    "    for i in range(5):\n",
    "        print(x[i])\n",
    "        print(y[i])\n",
    "        \n",
    "print_val(preds, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d1ec92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
