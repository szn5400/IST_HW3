{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "015484a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5548127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Define the One-hot Encoder\n",
    "ohe = preprocessing.OneHotEncoder()\n",
    "\n",
    "# Load MNIST data\n",
    "#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape data\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "# Fit and transform training data\n",
    "ohe.fit(y_train)\n",
    "transformed_train = ohe.transform(y_train).toarray()\n",
    "\n",
    "# Fit and transform testing data\n",
    "ohe.fit(y_test)\n",
    "transformed_test = ohe.transform(y_test).toarray()\n",
    "\n",
    "x_train = x_train[0:50000, :]\n",
    "#y_train = transformed_train[0:50000,:]\n",
    "transformed_train = transformed_train[0:50000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c01c7d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (50000, 28, 28)\n",
      "Y_train: (50000, 10)\n",
      "X_test:  (10000, 28, 28)\n",
      "Y_test:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print('X_train: ' + str(x_train.shape))\n",
    "print('Y_train: ' + str(transformed_train.shape))\n",
    "print('X_test:  '  + str(x_test.shape))\n",
    "print('Y_test:  '  + str(transformed_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfffe09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_mod = x_train.reshape(50000, 784)\n",
    "train_y_mod = transformed_train\n",
    "test_X_mod = x_test.reshape(10000, 784)\n",
    "test_y_mod = transformed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef204d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_input = 784\n",
    "size_hidden1 = 512\n",
    "size_hidden2 = 256\n",
    "size_output = 10\n",
    "number_of_train_examples = 50000\n",
    "number_of_test_examples = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0c332bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "X_train = tf.keras.utils.normalize(train_X_mod, axis=1)\n",
    "y_train = train_y_mod\n",
    "X_test = test_X_mod\n",
    "y_test = test_y_mod "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebbf266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "np.random.seed(43)\n",
    "tf.random.set_seed(43)\n",
    "# Split dataset into batches\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(128)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d82b23c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "  def __init__(self, size_input, size_hidden1, size_hidden2, size_output, device=None):\n",
    "    \"\"\"\n",
    "    size_input: int, size of input layer\n",
    "    size_hidden1: int, size of hidden layer 1\n",
    "    size_hidden2: int, size of hodden layer 2\n",
    "    size_output: int, size of output layer\n",
    "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
    "    \"\"\"\n",
    "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_output, self.device =\\\n",
    "    size_input, size_hidden1, size_hidden2, size_output, device\n",
    "    \n",
    "    # Initialize weights between input layer and hidden layer\n",
    "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1))\n",
    "    # Initialize biases for hidden layer\n",
    "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden1]))\n",
    "    # Initialize weights between input layer and hidden layer\n",
    "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
    "    # Initialize biases for hidden layer\n",
    "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden2]))\n",
    "     # Initialize weights between hidden layer and output layer\n",
    "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output],stddev=0.1))\n",
    "    # Initialize biases for output layer\n",
    "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
    "    \n",
    "    # Define variables to be updated during backpropagation\n",
    "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
    "    #print(len(self.variables))\n",
    "    #print(self.variables)\n",
    "    \n",
    "    self.t=0\n",
    "    '''\n",
    "    m1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1))\n",
    "    m2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
    "    m3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output],stddev=0.1))\n",
    "    m4 = tf.Variable(tf.random.normal([1, self.size_hidden1],stddev=0.1))\n",
    "    m5 = tf.Variable(tf.random.normal([1, self.size_hidden2],stddev=0.1))\n",
    "    m6 = tf.Variable(tf.random.normal([1, self.size_output],stddev=0.1))\n",
    "    self.mt = [m1,m2,m3,m4,m5,m6]\n",
    "    '''\n",
    "    \n",
    "    self.mt=[tf.zeros_like(self.W1,dtype=tf.float32), tf.zeros_like(self.W2,dtype=tf.float32),tf.zeros_like(self.W3,dtype=tf.float32),tf.zeros_like(self.b1,dtype=tf.float32),tf.zeros_like(self.b2,dtype=tf.float32),tf.zeros_like(self.b3,dtype=tf.float32)]\n",
    "    '''\n",
    "    m1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1))\n",
    "    m2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
    "    m3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output],stddev=0.1))\n",
    "    m4 = tf.Variable(tf.random.normal([1, self.size_hidden1],stddev=0.1))\n",
    "    m5 = tf.Variable(tf.random.normal([1, self.size_hidden2],stddev=0.1))\n",
    "    m6 = tf.Variable(tf.random.normal([1, self.size_output],stddev=0.1))\n",
    "    self.vt = [m1,m2,m3,m4,m5,m6]\n",
    "    '''\n",
    "    self.vt=[tf.zeros_like(self.W1,dtype=tf.float32), tf.zeros_like(self.W2,dtype=tf.float32),tf.zeros_like(self.W3,dtype=tf.float32),tf.zeros_like(self.b1,dtype=tf.float32),tf.zeros_like(self.b2,dtype=tf.float32),tf.zeros_like(self.b3,dtype=tf.float32)]\n",
    "    '''\n",
    "    m1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1))\n",
    "    m2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
    "    m3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output],stddev=0.1))\n",
    "    m4 = tf.Variable(tf.random.normal([1, self.size_hidden1],stddev=0.1))\n",
    "    m5 = tf.Variable(tf.random.normal([1, self.size_hidden2],stddev=0.1))\n",
    "    m6 = tf.Variable(tf.random.normal([1, self.size_output],stddev=0.1))\n",
    "    self.ut = [m1,m2,m3,m4,m5,m6]\n",
    "    '''\n",
    "    self.ut=[tf.zeros_like(self.W1,dtype=tf.float32), tf.zeros_like(self.W2,dtype=tf.float32),tf.zeros_like(self.W3,dtype=tf.float32),tf.zeros_like(self.b1,dtype=tf.float32),tf.zeros_like(self.b2,dtype=tf.float32),tf.zeros_like(self.b3,dtype=tf.float32)]\n",
    "    #print(self.mt)\n",
    "    \n",
    "  def forward(self, X):\n",
    "    \"\"\"\n",
    "    forward pass\n",
    "    X: Tensor, inputs\n",
    "    \"\"\"\n",
    "    if self.device is not None:\n",
    "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
    "        self.y = self.compute_output(X)\n",
    "    else:\n",
    "      self.y = self.compute_output(X)\n",
    "      \n",
    "    return self.y\n",
    "  \n",
    "  def loss(self, y_pred, y_true):\n",
    "    '''\n",
    "    y_pred - Tensor of shape (batch_size, size_output)\n",
    "    y_true - Tensor of shape (batch_size, size_output)\n",
    "    '''\n",
    "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
    "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "    #return tf.losses.mean_squared_error(y_true_tf, y_pred_tf)\n",
    "    #print(y_true_tf)\n",
    "    #print(y_pred_tf)\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "    loss_val = cce(y_true_tf, y_pred_tf)\n",
    "    #regularizer = tf.nn.l2_loss(self.W1)+tf.nn.l2_loss(self.W2)\n",
    "    #loss_val = tf.reduce_mean(loss_val + 0.01 * regularizer)\n",
    "    \n",
    "    #print(loss_val)\n",
    "    return loss_val\n",
    "\n",
    "  def accuracy(self, y_pred, y_true):\n",
    "    '''\n",
    "    y_pred - Tensor of shape (batch_size, size_output)\n",
    "    y_true - Tensor of shape (batch_size, size_output)\n",
    "    '''\n",
    "    '''\n",
    "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
    "    #y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "    y_pred_tf = tf.cast(tf.reshape(y_pred, (-1, self.size_output)), dtype=tf.float32)\n",
    "    print(y_true_tf)\n",
    "    print(y_pred_tf)\n",
    "    ## CALCULATING COST AND ACCURACY\n",
    "    #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_preds, labels=y))\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost)\n",
    "    correct_pred = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
    "    #print(correct_pred)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "    #print(ac)\n",
    "    return accuracy\n",
    "    '''\n",
    "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
    "    #y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "    y_pred_tf = tf.cast(tf.reshape(y_pred, (-1, self.size_output)), dtype=tf.float32)    \n",
    "    correct_pred = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
    "    #print(correct_pred)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "    #print(accuracy)\n",
    "    return accuracy\n",
    "  \n",
    "  def backward(self, X_train, y_train):\n",
    "    \"\"\"\n",
    "    backward pass\n",
    "    \"\"\"\n",
    "    '''\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2)\n",
    "    with tf.GradientTape() as tape:\n",
    "      predicted = self.forward(X_train)\n",
    "      current_loss = self.loss(predicted, y_train)\n",
    "    grads = tape.gradient(current_loss, self.variables)\n",
    "    optimizer.apply_gradients(zip(grads, self.variables))\n",
    "    '''\n",
    "    self.t=self.t+1\n",
    "    alpha = 1e-3\n",
    "    beta_1=0.9\n",
    "    beta_2=0.999\n",
    "    beta_3=0.999987\n",
    "    eps=1e-8\n",
    "    eps2 = 1e-1 \n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "      predicted = self.forward(X_train)\n",
    "      current_loss = self.loss(predicted, y_train)\n",
    "    grads = tape.gradient(current_loss, self.variables)\n",
    "    #grads = grads+eps2\n",
    "    #print(grads)\n",
    "    #print(len(grads[0]))\n",
    "    #print(len(grads[1]))\n",
    "    #print(len(self.m[0]))\n",
    "    #print(len(self.m[1]))\n",
    "\n",
    "    self.mt=[m_val*beta_1+(1-beta_1)*g for m_val, g in zip(self.mt, grads) ]\n",
    "    self.vt=[v_val*beta_3+(1-beta_3)*(g**3) for v_val, g in zip(self.vt, grads) ]\n",
    "    self.ut=[u_val*beta_2+(1-beta_2)*(g**2) for u_val, g in zip(self.ut, grads) ]\n",
    "    '''\n",
    "    print('MT')\n",
    "    print(self.mt)\n",
    "    print('VT')\n",
    "    print(self.vt)\n",
    "    print('UT')\n",
    "    print(self.ut)\n",
    "    '''\n",
    "\n",
    "    mt_hat=[m_val/(1-(beta_1**self.t)) for m_val in self.mt]\n",
    "    vt_hat=[v_val/(1-(beta_3**self.t)) for v_val in self.vt]\n",
    "    ut_hat=[u_val/(1-(beta_2**self.t)) for u_val in self.ut]\n",
    "    #print('MT_hat')\n",
    "    #print(self.mt)\n",
    "    #print('VT_hat')\n",
    "    #print(self.vt)\n",
    "    #print('UT_hat')\n",
    "    #print(self.ut)\n",
    "    \n",
    "    dws_new = [(alpha * m_val /(np.sqrt(np.abs(v_val))+(np.power(np.abs(u_val),0.3333)*eps)+eps2)) for m_val, v_val, u_val in zip(mt_hat,vt_hat,ut_hat)]\n",
    "    #dws_new = [(alpha * m_val /(eps2)) for m_val, v_val, u_val in zip(mt_hat,vt_hat,ut_hat)]\n",
    "    #print('dws_new')\n",
    "    #print(dws_new)\n",
    "    Wt = [wt - d for wt,d in zip(self.variables, dws_new)]\n",
    "    #print(Wt)\n",
    "    W1 = self.variables[0]\n",
    "    W1.assign(Wt[0])\n",
    "    W2 = self.variables[1]\n",
    "    W2.assign(Wt[1])\n",
    "    W3 = self.variables[2]\n",
    "    W3.assign(Wt[2])\n",
    "    b1 = self.variables[3]\n",
    "    b1.assign(Wt[3])\n",
    "    b2 = self.variables[4]\n",
    "    b2.assign(Wt[4])\n",
    "    b3 = self.variables[5]\n",
    "    b3.assign(Wt[5])\n",
    "        \n",
    "        \n",
    "  def compute_output(self, X):\n",
    "    \"\"\"\n",
    "    Custom method to obtain output tensor during forward pass\n",
    "    \"\"\"\n",
    "    # Cast X to float32\n",
    "    X_tf = tf.cast(X, dtype=tf.float32)\n",
    "    #Remember to normalize your dataset before moving forward\n",
    "    # Compute values in hidden layer1\n",
    "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
    "    hhat1 = tf.nn.relu(what1)\n",
    "    #hhat1_1 = tf.nn.dropout(hhat1, keep_prob)\n",
    "    # Compute values in hidden layer2\n",
    "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
    "    hhat2 = tf.nn.relu(what2)\n",
    "    # Compute output\n",
    "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
    "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
    "    #Second add tf.Softmax(output) and then return this variable\n",
    "    output = tf.nn.softmax(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39645ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of epochs\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8eba428e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Epoch = 1 - Average celoss:= 0.0645565673828125- Acc:= 0.6411207318305969 \n",
      "Number of Epoch = 2 - Average celoss:= 0.02705192626953125- Acc:= 0.8454984426498413 \n",
      "Number of Epoch = 3 - Average celoss:= 0.02171716796875- Acc:= 0.8723771572113037 \n",
      "Number of Epoch = 4 - Average celoss:= 0.019568966064453124- Acc:= 0.8834976553916931 \n",
      "Number of Epoch = 5 - Average celoss:= 0.018333040771484375- Acc:= 0.8914960026741028 \n",
      "Number of Epoch = 6 - Average celoss:= 0.017445733642578124- Acc:= 0.8967365026473999 \n",
      "Number of Epoch = 7 - Average celoss:= 0.01677907958984375- Acc:= 0.9000555872917175 \n",
      "Number of Epoch = 8 - Average celoss:= 0.016264759521484377- Acc:= 0.9032154083251953 \n",
      "Number of Epoch = 9 - Average celoss:= 0.015843240966796877- Acc:= 0.9062144756317139 \n",
      "Number of Epoch = 10 - Average celoss:= 0.015335631103515626- Acc:= 0.9090354442596436 \n",
      "Number of Epoch = 11 - Average celoss:= 0.01492930908203125- Acc:= 0.9116344451904297 \n",
      "Number of Epoch = 12 - Average celoss:= 0.014534595947265624- Acc:= 0.9140554666519165 \n",
      "Number of Epoch = 13 - Average celoss:= 0.0141229833984375- Acc:= 0.9160937666893005 \n",
      "Number of Epoch = 14 - Average celoss:= 0.01375500732421875- Acc:= 0.9184545874595642 \n",
      "Number of Epoch = 15 - Average celoss:= 0.013375517578125- Acc:= 0.9206333160400391 \n",
      "Number of Epoch = 16 - Average celoss:= 0.013038563232421875- Acc:= 0.9228950142860413 \n",
      "Number of Epoch = 17 - Average celoss:= 0.012654737548828125- Acc:= 0.9244938492774963 \n",
      "Number of Epoch = 18 - Average celoss:= 0.012337607421875- Acc:= 0.9273539185523987 \n",
      "Number of Epoch = 19 - Average celoss:= 0.0119915966796875- Acc:= 0.9289335012435913 \n",
      "Number of Epoch = 20 - Average celoss:= 0.011709229736328125- Acc:= 0.9303536415100098 \n",
      "\n",
      "Total time taken (in seconds): 2834.73\n"
     ]
    }
   ],
   "source": [
    "# Initialize model using CPU\n",
    "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_output, device='cpu')\n",
    "\n",
    "# Array to store accuracy and loss\n",
    "loss_with_epoch = []\n",
    "acc_with_epoch = []\n",
    "\n",
    "time_start = time.time()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "  ac = 0\n",
    "  count = 0\n",
    "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
    "  lt = 0\n",
    "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(1234)).batch(20)\n",
    "  for inputs, outputs in train_ds:\n",
    "    preds = mlp_on_cpu.forward(inputs)\n",
    "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
    "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
    "    mlp_on_cpu.backward(inputs, outputs)\n",
    "    ac = ac+mlp_on_cpu.accuracy(preds, outputs)\n",
    "    #ac = mlp_on_cpu.accuracy(preds, outputs)\n",
    "    count += 1\n",
    "  print('Number of Epoch = {} - Average celoss:= {}- Acc:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0], ac/count))\n",
    "  loss_with_epoch.append(np.sum(loss_total) / X_train.shape[0])\n",
    "  acc_with_epoch.append(ac/count)\n",
    "time_taken = time.time() - time_start\n",
    "\n",
    "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
    "#For per epoch_time = Total_Time / Number_of_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a3299ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = [i for i in range(1,21)]\n",
    "line1, = plt.plot(x, acc_with_epoch, label='accr')\n",
    "line2, = plt.plot(x, loss_with_epoch, label='ce loss')\n",
    "#plt.plot(x, acc_with_epoch, label='accr')\n",
    "plt.legend(handles=[line1, line2], loc='best')\n",
    "plt.grid(b=True, color='aqua', alpha=0.6, linestyle='dashdot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e49268b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test celoss: 0.1216 and accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
    "#test_loss_total = 0.0\n",
    "for inputs, outputs in test_ds:\n",
    "  preds = mlp_on_cpu.forward(inputs)\n",
    "  #b = mlp_on_default.loss(preds, outputs)\n",
    "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
    "  ac = mlp_on_cpu.accuracy(preds, outputs)\n",
    "# a = (test_loss_total.numpy() / X_train.shape[0])\n",
    "# print(X_train.shape[0])\n",
    "# print(test_loss_total.numpy())\n",
    "# print(b)\n",
    "print('Test celoss: {:.4f} and accuracy: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_train.shape[0], ac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab00449",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
